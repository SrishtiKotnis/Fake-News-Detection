{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import getEmbeddings2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "def plot_cmat(yte, ypred):\n",
    "    '''Plotting confusion matrix'''\n",
    "    skplt.plot_confusion_matrix(yte, ypred)\n",
    "    plt.show()\n",
    "\n",
    "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
    "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./yte_shuffled.npy'):\n",
    "    getEmbeddings2.clean_data()\n",
    "\n",
    "\n",
    "xtr = np.load('./xtr_shuffled.npy')\n",
    "xte = np.load('./xte_shuffled.npy')\n",
    "y_train = np.load('./ytr_shuffled.npy')\n",
    "y_test = np.load('./yte_shuffled.npy')\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160064    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               106400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 266,665\n",
      "Trainable params: 266,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16059 samples, validate on 4153 samples\n",
      "Epoch 1/5\n",
      "16059/16059 [==============================] - 691s 43ms/step - loss: 0.3442 - accuracy: 0.8570 - val_loss: 0.1752 - val_accuracy: 0.9367\n",
      "Epoch 2/5\n",
      "16059/16059 [==============================] - 808s 50ms/step - loss: 0.1558 - accuracy: 0.9461 - val_loss: 0.2151 - val_accuracy: 0.9229\n",
      "Epoch 3/5\n",
      "16059/16059 [==============================] - 818s 51ms/step - loss: 0.1488 - accuracy: 0.9466 - val_loss: 0.1777 - val_accuracy: 0.9364\n",
      "Epoch 4/5\n",
      "16059/16059 [==============================] - 710s 44ms/step - loss: 0.0937 - accuracy: 0.9695 - val_loss: 0.1588 - val_accuracy: 0.9465\n",
      "Epoch 5/5\n",
      "16059/16059 [==============================] - 847s 53ms/step - loss: 0.0832 - accuracy: 0.9732 - val_loss: 0.2259 - val_accuracy: 0.9319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2296e7ea908>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"start\")\n",
    "# Create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_num, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 500, 32)           160064    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 496, 64)           10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 124, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 236,469\n",
      "Trainable params: 236,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 16059 samples, validate on 4153 samples\n",
      "Epoch 1/2\n",
      "16059/16059 [==============================] - 104s 7ms/step - loss: 0.2712 - accuracy: 0.8775 - val_loss: 0.1585 - val_accuracy: 0.9456\n",
      "Epoch 2/2\n",
      "16059/16059 [==============================] - 106s 7ms/step - loss: 0.0932 - accuracy: 0.9667 - val_loss: 0.1336 - val_accuracy: 0.9538\n",
      "4153/4153 [==============================] - 8s 2ms/step\n",
      "Test score: 0.13361654052985825\n",
      "Test accuracy: 0.9537683725357056\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "# CNN-LSTM\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nembedding_vecor_length = 32\\nmodel = Sequential()\\n\\n\\n\\nmodel.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\\nmodel.add(Dropout(0.25))\\nmodel.add(LSTM(100))\\n\\nmodel.add(Dense(1))\\nmodel.add(Activation(\\'sigmoid\\'))\\n\\nmodel.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])\\nprint(model.summary())\\n\\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\\nscore, acc = model.evaluate(X_test, y_test, batch_size=batch_size)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot1.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
